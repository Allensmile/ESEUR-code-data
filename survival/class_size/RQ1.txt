
options(digits=3)

inputFile = "Eclipse"
fileExt = ".csv"

# Read in the data
input <- read.table(paste(inputFile, fileExt, sep=""), header=T, sep=",")
# order data by file size (makes index-based residual analysis easier)
input <- input[order(input$size),]
# event of interest is a either a defect introduction (input$intro) or defect fix (input$fix)
input$event <- input$intro
# state is the number of defects previously introduced (either 0, 1, 2, 3, 4, 5-10, 10-20, 20+)
input$state[which(input$state >= 5 & input$state < 10)] = 6
input$state[which(input$state >= 10 & input$state < 20)] = 7
input$state[which(input$state > 20)] = 8
input$logSize <- log(input$size)
input$logSize[which(input$logSize==-Inf)] = 0

sink("RQ1.txt")

# ----- ----- ----- ----- ----- ----- -----      SET UP       ----- ----- ----- ----- ----- ----- -----
# ----- ----- ----- ----- ----- ----- -----   LINK FUNCTION   ----- ----- ----- ----- ----- ----- -----
# Determine the functional form (link function) of each covariate that we wish to model.

# pdf("Link Functions.pdf")
# Plot size against log-relative risk to identify the link function
fit <- coxph(Surv(start,end,event) ~ ns(size, df=4) + cluster(id), data=input)
pred <- predict(fit, type="terms", se=TRUE)
hmat <- cbind(pred$fit[,1], pred$fit[,1]+2*pred$se[,1], pred$fit[,1]-2*pred$se[,1])
matplot(input$size[order(input$size)], hmat[order(input$size),], 
	xlab = "LOC", ylab="Log Relative Risk", 
	col=c("BLACK","BLACK","BLACK"), lty=c(1,2,2), type="l")
# From the previous figure, the link function appears to be the log function. We must verify that the 
# link function is the log function by plotting the log of the covariate "size" against log-relative 
# risk to identify a linear function
fit <- coxph(Surv(start,end,event) ~ ns(logSize, df=4) + cluster(id), data=input)
pred <- predict(fit, type="terms", se=TRUE)
hmat <- cbind(pred$fit[,1], pred$fit[,1]+2*pred$se[,1], pred$fit[,1]-2*pred$se[,1])
matplot(input$logSize[order(input$logSize)], hmat[order(input$logSize),], 
	xlab = "log (LOC)", ylab="Log Relative Risk", 
	col=c("BLACK","BLACK","BLACK"), lty=c(1,2,2), type="l")
# The relationship between log(size) and log-relative risk is not linear. over the entire LOC range. 
# Therefore, we should divide and analysis our data seperatly
# Split the data given the selected link function
LOC_partition_point1 = 4.25
LOC_partition_point2 = 6
abline(v=LOC_partition_point1)
abline(v=LOC_partition_point2)
dev.off()

# ----- ----- ----- ----- ----- ----- -----   LINK FUNCTION   ----- ----- ----- ----- ----- ----- -----
# ----- ----- ----- ----- ----- ----- -----   DATA PARTITION  ----- ----- ----- ----- ----- ----- -----

# Split the data into three partitions given the link function 
# Small partition of log (LOC)
inputS <- subset(input, input$logSize <= LOC_partition_point1)
# Middle partition of log (LOC)
inputM <- subset(input, input$logSize > LOC_partition_point1 & input$logSize < LOC_partition_point2)
# Large partition of log (LOC)
inputL <- subset(input, input$logSize >= LOC_partition_point2)

# ----- ----- ----- ----- ----- ----- -----   DATA PARTITION  ----- ----- ----- ----- ----- ----- -----
# ----- ----- ----- ----- ----- ----- -----     COX MODELS    ----- ----- ----- ----- ----- ----- -----
# Build a Cox model for each partition
coxFitS <- coxph(Surv(start,end,event) ~ logSize + strata(state) + cluster(id), data=inputS)
print(coxFitS)
coxFitM <- coxph(Surv(start,end,event) ~ logSize + strata(state) + cluster(id), data=inputM)
print(coxFitM)
coxFitL <- coxph(Surv(start,end,event) ~ logSize + strata(state) + cluster(id), data=inputL)
print(coxFitL)

# ----- ----- ----- ----- ----- ----- -----     COX MODELS    ----- ----- ----- ----- ----- ----- -----
# ----- ----- ----- ----- ----- ----- -----   PH ASSUMPTION   ----- ----- ----- ----- ----- ----- -----
# Verify that the Proportional Hazards assumption holds for each Cox model.  Both the numerical and 
# graphical technique are based on the scaled Schoenfeld Residuals.

# Numerical technique
coxPHAS <- cox.zph(coxFitS)
print(coxPHAS)
coxPHAM <- cox.zph(coxFitM)
print(coxPHAM)
coxPHAL <- cox.zph(coxFitL)
print(coxPHAL)

# Graphical technique - scaled Schoenfeld Residuals
pdf("scaled Schoenfeld Residuals.pdf")
# I have modified the plot.cox.zph function for purely graphical reasons.  It is much easier to 
# distinguish the grey residuals from the black smoothed spline.
my.plot.cox.zph(coxPHAS, df=2)
my.plot.cox.zph(coxPHAM)
my.plot.cox.zph(coxPHAL)
dev.off()

# ----- ----- ----- ----- ----- ----- -----   PH ASSUMPTION   ----- ----- ----- ----- ----- ----- -----
# ----- ----- ----- ----- ----- ----- -----    MARTINGALE     ----- ----- ----- ----- ----- ----- -----
# Plot the martingale residuals against the covariates of a Cox model.   

pdf("Martingale Residuals.pdf")
# Plot Martingale residuals for the small partition
scatter.smooth(inputS$logSize, coxFitS$resid, xlab="log (inputS$size)", ylab="residuals", col="GRAY")
abline(h=0, lty=2)
# Plot Martingale residuals for the middle partition
scatter.smooth(inputM$logSize, coxFitM$resid, xlab="log (inputM$size)", ylab="residuals", col="GRAY")
abline(h=0, lty=2)
# Plot Martingale residuals for the large partition
scatter.smooth(inputL$logSize, coxFitL$resid, xlab="log (inputL$size)", ylab="residuals", col="GRAY")
abline(h=0, lty=2)
dev.off()

# ----- ----- ----- ----- ----- ----- -----    MARTINGALE     ----- ----- ----- ----- ----- ----- -----
# ----- ----- ----- ----- ----- ----- -----      DFBETA       ----- ----- ----- ----- ----- ----- -----
# Plot the martingale residuals against the covariates of a Cox model.   

pdf("dfbeta Residuals.pdf")
# Plot dfbeta residuals for the small partition
dfbetaS <- resid(coxFitS, collapse=inputS$id, type="dfbeta")
plot(dfbetaS, ylab=paste("dfbeta for", names(coef(coxFitS))[1]), col="GRAY")
abline(h=0, lty=2)
# Plot dfbeta residuals for the middle partition
dfbetaM <- resid(coxFitM, collapse=inputM$id, type="dfbeta")
plot(dfbetaM, ylab=paste("dfbeta for", names(coef(coxFitM))[1]), col="GRAY")
abline(h=0, lty=2)
# Plot dfbeta residuals for the large partition
dfbetaL <- resid(coxFitS, collapse=inputS$id, type="dfbeta")
plot(dfbetaL, ylab=paste("dfbeta for", names(coef(coxFitL))[1]), col="GRAY")
abline(h=0, lty=2)
dev.off()

# Remove overly influential data points that inpact the coefficient value by at least 5%.  Then return 
# to the COX MODEL section of the script and repeat the analysis.  When there are no overly influential
# data points left, proceed to SPEARMAN section.
# Remove overly influential data points from the small partition
indexLinesS <- which(abs(dfbetaS) > 0.05*abs(coxFitS$coeff[[1]]))
if(length(indexLinesS) > 0)
	inputS[indexLinesS,] <- rep(NA,length(indexLinesS))
inputS <- na.omit(inputS)
# Remove overly influential data points from the middle partition
indexLinesM <- which(abs(dfbetaM) > 0.05*abs(coxFitM$coeff[[1]]))
if(length(indexLinesM) > 0)
	inputM[indexLinesM,] <- rep(NA,length(indexLinesM))
inputM <- na.omit(inputM)
# Remove overly influential data points from the large partition
indexLinesL <- which(abs(dfbetaL) > 0.05*abs(coxFitL$coeff[[1]]))
if(length(indexLinesL) > 0)
	inputL[indexLinesL,] <- rep(NA,length(indexLinesL))
inputL <- na.omit(inputL)

# ----- ----- ----- ----- ----- ----- -----      DFBETA       ----- ----- ----- ----- ----- ----- -----
# ----- ----- ----- ----- ----- ----- -----     SPEARMAN      ----- ----- ----- ----- ----- ----- -----
# Spearman correlation between expected and actual events

# Small partition
expectedS = predict(coxFitS, type="expected", collapse=inputS$id)
actualS = tapply(t(inputS$event),(inputS$id),sum)
actualS = actualS[apply(actualS,1,function(x) {!is.na(x)})]
print(cor(actualS, expectedS, method="spearman"))
# Middle partition
expectedM = predict(coxFitM, type="expected", collapse=inputM$id)
actualM = tapply(t(inputM$event),(inputM$id),sum)
actualM = actualM[apply(actualM,1,function(x) {!is.na(x)})]
print(cor(actualM, expectedM, method="spearman"))
# Large partition
expectedL = predict(coxFitL, type="expected", collapse=inputL$id)
actualL = tapply(t(inputL$event),(inputL$id),sum)
actualL = actualL[apply(actualL,1,function(x) {!is.na(x)})]
print(cor(actualL, expectedL, method="spearman"))
# ----- ----- ----- ----- ----- ----- -----     SPEARMAN      ----- ----- ----- ----- ----- ----- -----
# ----- ----- ----- ----- ----- ----- -----  PARTITION STATS  ----- ----- ----- ----- ----- ----- -----
# Print partition statistics
print("Small Partition")
print(length(unique(inputS$id)))
print(length(unique(inputS$id))/length(unique(input$id)))
print(sum(inputS$event))
print(sum(inputS$event)/sum(input$event))
print("Middle Partition")
print(length(unique(inputM$id)))
print(length(unique(inputM$id))/length(unique(input$id)))
print(sum(inputM$event))
print(sum(inputM$event)/sum(input$event))
print("Large Partition")
print(length(unique(inputL$id)))
print(length(unique(inputL$id))/length(unique(input$id)))
print(sum(inputL$event))
print(sum(inputL$event)/sum(input$event))

# ----- ----- ----- ----- ----- ----- -----  PARTITION STATS  ----- ----- ----- ----- ----- ----- -----
# ----- ----- ----- ----- ----- ----- -----     TEAR DOWN     ----- ----- ----- ----- ----- ----- -----
sink()

# ----- ----- ----- ----- ----- ----- -----     TEAR DOWN     ----- ----- ----- ----- ----- ----- -----

# ----- ----- ----- ----- ----- ----- -----       OTHER       ----- ----- ----- ----- ----- ----- -----
# Need to remove objects when they are no longer needed (TODO)
#rm(list = ls(all = TRUE)) 

# My plot.cox.zph
my.plot.cox.zph <- function(x, resid=TRUE, se=TRUE, df=4, nsmo=40) {
	xx <- x$x
	yy <- x$y
	d <- nrow(yy)
	df <- max(df)     #error proofing
	nvar <- ncol(yy)
	var <- 1:nvar
	pred.x <- seq(from=min(xx), to=max(xx), length=nsmo)
	temp <- c(pred.x, xx)
	lmat <- ns(temp, df=df, intercept=TRUE)
	pmat <- lmat[1:nsmo,]       # for prediction
	xmat <- lmat[-(1:nsmo),]
	qmat <- qr(xmat)
	if (qmat$rank < df) {
		stop("Spline fit is singular, try a smaller degrees of freedom")
	}

	if (se) {
		bk <- backsolve(qmat$qr[1:df, 1:df], diag(df))
		xtx <- bk %*% t(bk)
		seval <- d*((pmat%*% xtx) *pmat) %*% rep(1, df)
	}

	ylab <- paste("Beta(t) for", dimnames(yy)[[2]])
	
	if (x$transform == 'log') {
		xx <- exp(xx)
		pred.x <- exp(pred.x)
	} else if (x$transform != 'identity') {
		xtime <- as.numeric(dimnames(yy)[[1]])
		indx <- !duplicated(xx)  #avoid a warning message in R
		apr1  <- approx(xx[indx], xtime[indx], seq(min(xx), max(xx), length=17)[2*(1:8)])
		temp <- signif(apr1$y,2)
		apr2  <- approx(xtime[indx], xx[indx], temp)
		xaxisval <- apr2$y
		xaxislab <- rep("",8)
		for (i in 1:8) xaxislab[i] <- format(temp[i])
	}

	for (i in var) {
		y <- yy[,i]
		yhat <- pmat %*% qr.coef(qmat, y)
		if (resid) {
			yr <-range(yhat, y)
		} else {
		      yr <-range(yhat)
		}
		if (se) {
			temp <- 2* sqrt(x$var[i,i]*seval)
			yup <- yhat + temp
			ylow<- yhat - temp
			yr <- range(yr, yup, ylow)
		}

		if (x$transform=='identity') {
			plot(range(xx), yr, type='n', xlab="Time", ylab=ylab[i], col="GREY")
		} else if (x$transform=='log') {
			plot(range(xx), yr, type='n', xlab="Time", ylab=ylab[i], log='x',	col="GREY")
		} else {
			plot(range(xx), yr, type='n', xlab="Time", ylab=ylab[i], axes=FALSE, col="GREY")
			axis(1, xaxisval, xaxislab)
			axis(2)
			box()
		}
		if (resid) {
			points(xx, y, col="GREY") 
		}
		lines(pred.x, yhat, col="BLACK")
		if (se) {
			lines(pred.x, yup,lty=2, col="BLACK")
			lines(pred.x, ylow, lty=2, col="BLACK")
		}
	}
}
# ----- ----- ----- ----- ----- ----- -----       OTHER       ----- ----- ----- ----- ----- ----- -----
